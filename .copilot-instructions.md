# ARIA Copilot Instructions — ASR, LLM, and Local TTS (Sonos)

Use the instructions defined in `$HOME/dev/aria/.copilot-instructions.md` as the authoritative project specification.

This task has **THREE parts**:

1. macOS microphone streaming client  
2. Server-side processing of FINAL transcribed text using a local LLM  
3. Server-side TTS output to Sonos (local-only, low latency)

> **Important**
> - Do NOT change existing, working audio behavior.
> - All processing must remain local (no cloud or external services).

---

## PART 1 — macOS MICROPHONE STREAMING CLIENT

Create the macOS microphone streaming client at:

```
client/macos/mic_stream.py
```

### IMPORTANT CONTEXT (DO NOT IGNORE)
- `ffmpeg` + AVFoundation caused audio timing issues (accelerated speech)
- SoX `rec` worked correctly
- Therefore: **DO NOT use ffmpeg or avfoundation**
- Use **Python + PortAudio** instead

### REQUIREMENTS

#### Audio capture
- Use `sounddevice` (PortAudio backend)
- Select the default system microphone automatically
- Capture at the device’s **native sample rate** (query it, do NOT force 16k at capture)
- Use `float32` internally

#### Resampling
- Use `soxr` for high-quality resampling
- Resample audio from device rate → **16000 Hz**
- Convert to PCM signed 16-bit little-endian before sending

#### Streaming
- Use `asyncio` + `websockets`
- Connect to: `ws://HOST:PORT/ws/asr`
- On connect, send JSON:

```json
{
  "type": "start",
  "sample_rate": 16000,
  "channels": 1,
  "format": "pcm_s16le",
  "chunk_ms": 40
}
```

- Then stream binary PCM frames
- Drop audio frames if websocket backpressure occurs (**do NOT buffer unbounded**)
- On Ctrl+C, send `{"type":"stop"}` and close cleanly

#### Chunking
- Target ~40 ms chunks at 16 kHz
- Ensure chunk boundaries are correct after resampling

#### Client Output
- Print partial transcripts as they arrive
- Print final transcripts on completion
- Prefix output with **`ARIA.TRANSCRIPT`**

#### Structure
- No GUI
- CLI usage:
  ```
  python mic_stream.py ws://localhost:8000/ws/asr
  ```
- Clean, readable, production-quality code
- Add comments explaining:
  - why capture is at native rate
  - why resampling is done client-side
  - why ffmpeg is not used

#### Dependencies
- sounddevice
- soxr
- numpy
- websockets

Also create:
```
client/macos/requirements.txt
```

#### Do NOT
- use ffmpeg
- hardcode device IDs
- hardcode sample rates except 16000 as output
- block the event loop

---

## PART 2 — SERVER-SIDE PROCESSING OF TRANSCRIBED TEXT

### Behavior
- ONLY FINAL transcripts (not partials) are processed
- Final transcript text is sent to a local LLM
- The LLM response is printed to the server console
- No TTS or audio playback yet (console output only)

### LLM ARCHITECTURE REQUIREMENTS
- Use a local LLM served over HTTP (default: Ollama)
- Do NOT embed model weights in the ARIA process
- Communication via HTTP POST only
- The LLM call must be **non-blocking** with respect to audio streaming

### LLM CONFIGURATION (ENV)
```
ARIA_LLM_PROVIDER=ollama
ARIA_LLM_URL=http://localhost:11434
ARIA_LLM_MODEL=qwen2.5:3b-instruct
```

### SERVER IMPLEMENTATION

Create:
```
app/llm/ollama_client.py
```

Responsibilities:
- send a prompt to the local LLM
- return generated text
- handle timeouts and HTTP errors gracefully

Prompting rules:
- System: *"You are ARIA, a local assistant running on a private server."*
- User: final transcribed text
- Keep responses concise (1–3 sentences max)

### Integration point
When ASR emits a FINAL transcript:

1. Print transcript  
2. Call LLM  
3. Print LLM output  

Console format:
```
ARIA.TRANSCRIPT: <text>
ARIA.LLM: <llm_response>
```

### Non-functional
- LLM processing must NOT block audio ingestion
- LLM errors must NOT crash the server
- LLM must be easy to disable via env var

---

## PART 3 — SERVER-SIDE TTS OUTPUT TO SONOS (LOCAL ONLY)

### Objective
- Convert the LLM response text into speech
- Output speech to a Sonos speaker via existing **AirPlay integration**
- Entirely local (no cloud services)
- Target low perceived latency using **chunked synthesis**
- Overriding Sonos playback is acceptable (no resume required)

---

### TTS ENGINE REQUIREMENTS
- Default engine: **Piper** (local, CPU-based)
- No cloud TTS services
- Engine invoked via subprocess (non-blocking)
- Output WAV PCM 16-bit (preferred)

### TTS CONFIGURATION (ENV)
```
ARIA_TTS_ENABLED=1
ARIA_TTS_ENGINE=piper
ARIA_TTS_VOICE=<voice_id_or_path>
ARIA_TTS_RATE=1.0
ARIA_TTS_CHUNKING=1
ARIA_TTS_MAX_CHARS_PER_CHUNK=220
ARIA_TTS_MIN_CHUNK_CHARS=60
ARIA_TTS_SINK=airplay
ARIA_TTS_AIRPLAY_TARGET=<sonos_device>
ARIA_TTS_CACHE_DIR=/tmp/aria_tts_cache
ARIA_TTS_TIMEOUT_SEC=10
```

---

### MODULES TO CREATE

#### 1. `app/tts/tts_engine.py`
- Interface:
  - `synthesize(text: str) -> TtsAudio`
- Responsible only for speech synthesis

#### 2. `app/tts/piper_engine.py`
- Piper-based implementation
- Cache audio by hash(text + voice + rate)
- Enforce timeouts
- Fail gracefully

#### 3. `app/tts/text_chunker.py`
- Split text into speakable chunks
- Prefer sentence boundaries
- Replace code blocks with short spoken summaries
- Avoid speaking long URLs

#### 4. `app/audio_sink/base.py`
- Sink interface:
  - `play_file(path)`
  - `stop(handle)`
  - `stop_all()`

#### 5. `app/audio_sink/airplay_sink.py`
- Wrap existing AirPlay integration
- Play WAV files to Sonos
- Support interrupt/stop
- Non-blocking

#### 6. `app/speech_output/speech_output.py`
- Orchestrates:
  - chunking
  - TTS generation
  - sequential playback
  - barge-in interrupt
- Async-friendly
- Public API:
  - `speak(text)`
  - `interrupt()`

---

### Integration Flow

On FINAL transcript:
1. Print transcript
2. Call LLM
3. Print LLM output
4. If `ARIA_TTS_ENABLED=1`:
   - `speech_output.speak(llm_response)`

Optional logs:
```
ARIA.TTS: speaking <n> chunks
ARIA.TTS.ERROR: <error>
```

---

### BARGE-IN REQUIREMENT
- On user speech start (VAD):
  - Stop current TTS playback immediately
  - Clear queued chunks
- No attempt to restore previous Sonos playback

---

### Non-functional
- Entirely local
- Errors must not crash the server
- Easy to disable via env var
- Low latency via:
  - chunking
  - caching
  - pipelined generation/playback
