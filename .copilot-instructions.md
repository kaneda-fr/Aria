# ARIA Copilot Instructions — ASR, LLM, and Local TTS (Sonos)

This task has **THREE parts**:

1. macOS microphone streaming client  
2. Server-side processing of FINAL transcribed text using a local LLM  
3. Server-side TTS output to Sonos (local-only, low latency)

> **Important**
> - Do NOT change existing, working audio behavior.
> - All processing must remain local (no cloud or external services).

---

## PART 1 — macOS MICROPHONE STREAMING CLIENT

Create the macOS microphone streaming client at:

```
client/macos/mic_stream.py
```

### IMPORTANT CONTEXT (DO NOT IGNORE)
- `ffmpeg` + AVFoundation caused audio timing issues (accelerated speech)
- SoX `rec` worked correctly
- Therefore: **DO NOT use ffmpeg or avfoundation**
- Use **Python + PortAudio** instead

### REQUIREMENTS

#### Audio capture
- Use `sounddevice` (PortAudio backend)
- Select the default system microphone automatically
- Capture at the device’s **native sample rate** (query it, do NOT force 16k at capture)
- Use `float32` internally

#### Resampling
- Use `soxr` for high-quality resampling
- Resample audio from device rate → **16000 Hz**
- Convert to PCM signed 16-bit little-endian before sending

#### Streaming
- Use `asyncio` + `websockets`
- Connect to: `ws://HOST:PORT/ws/asr`
- On connect, send JSON:

```json
{
  "type": "start",
  "sample_rate": 16000,
  "channels": 1,
  "format": "pcm_s16le",
  "chunk_ms": 40
}
```

- Then stream binary PCM frames
- Drop audio frames if websocket backpressure occurs (**do NOT buffer unbounded**)
- On Ctrl+C, send `{"type":"stop"}` and close cleanly

#### Chunking
- Target ~40 ms chunks at 16 kHz
- Ensure chunk boundaries are correct after resampling

#### Client Output
- The client does not receive transcripts; server is the source of truth for ASR/LLM logs.

#### Structure
- No GUI
- CLI usage:
  ```
  python mic_stream.py ws://localhost:8000/ws/asr
  ```
- Clean, readable, production-quality code
- Add comments explaining:
  - why capture is at native rate
  - why resampling is done client-side
  - why ffmpeg is not used

#### Dependencies
- sounddevice
- soxr
- numpy
- websockets

Also create:
```
client/macos/requirements.txt
```

#### Do NOT
- use ffmpeg
- hardcode device IDs
- hardcode sample rates except 16000 as output
- block the event loop

---

## PART 2 — SERVER-SIDE PROCESSING OF TRANSCRIBED TEXT

### Behavior
- ONLY FINAL transcripts (not partials) are processed
- Final transcript text is sent to a local LLM
- The LLM response is printed to the server console
- No TTS or audio playback yet (console output only)

### LLM ARCHITECTURE REQUIREMENTS
- Use a local LLM served over HTTP (default: Ollama)
- Do NOT embed model weights in the ARIA process
- Communication via HTTP POST only
- The LLM call must be **non-blocking** with respect to audio streaming

### LLM CONFIGURATION (ENV)
```
ARIA_LLM_PROVIDER=ollama
ARIA_LLM_URL=http://localhost:11434
ARIA_LLM_MODEL=qwen2.5:3b-instruct
```

### SERVER IMPLEMENTATION

Create:
```
app/llm/ollama_client.py
```

Responsibilities:
- send a prompt to the local LLM
- return generated text
- handle timeouts and HTTP errors gracefully

Prompting rules:
- System: *"You are ARIA, a local assistant running on a private server."*
- User: final transcribed text
- Keep responses concise (1–3 sentences max)

### Integration point
When ASR emits a FINAL transcript:

1. Print transcript  
2. Call LLM  
3. Print LLM output  

Console format:
```
ARIA.TRANSCRIPT: <text>
ARIA.LLM: <llm_response>
```

### Non-functional
- LLM processing must NOT block audio ingestion
- LLM errors must NOT crash the server
- LLM must be easy to disable via env var

---

## PART 3 — SERVER-SIDE TTS OUTPUT TO SONOS (LOCAL ONLY, LOW LATENCY, HTTP)

### Architecture choice: **Option 1 — Single Server**
The **same Aria server process** that receives microphone audio and runs ASR/LLM must also:
- host a small **HTTP audio endpoint** that Sonos can fetch from (pull model)
- control Sonos playback via **SoCo** (`play_uri`, `stop`)

Aria runs as a **single server process** that handles:
- microphone audio input (existing)
- VAD / STT
- LLM
- TTS
- HTTP audio endpoint for Sonos output
- Sonos control via SoCo

**Do not** use AirPlay/RAOP for Aria speech output (too much buffering / latency).

**Mental model**
- Mic audio is **pushed** to Aria (client → server)
- TTS audio is **pulled** by Sonos (Sonos → Aria HTTP)

Sonos audio is NOT streamed live. Sonos pulls short WAV clips over HTTP.

---

### Objective
- Convert the LLM response text into speech
- Output speech to a Sonos speaker using **Sonos native HTTP playback**:
  1) Aria generates a Sonos-friendly WAV file (cached)
  2) Aria serves it over HTTP
  3) Aria tells Sonos to play the URL via SoCo `play_uri(url)`
- Entirely local (no cloud services)
- Target low perceived latency using **chunked synthesis**
- Overriding Sonos playback is acceptable (no resume required)

---

### TTS ENGINE REQUIREMENTS
- Default engine: **Piper** (local, CPU-based)
- No cloud TTS services
- Engine invoked via subprocess (non-blocking / async-friendly)
- Output WAV PCM 16-bit (preferred)

### AUDIO FORMAT REQUIREMENTS (Sonos-friendly)
Ensure the audio served to Sonos is:
- WAV PCM **s16le**
- **stereo**
- **44100 Hz**
- Optional gain scaling via `volume` parameter (default 0.3)

Use `ffmpeg` to normalize Piper output into the required format.

---

### TTS CONFIGURATION (ENV)
```
ARIA_TTS_ENABLED=1
ARIA_TTS_ENGINE=piper
ARIA_TTS_VOICE=<voice_id_or_path_or_model_path>

# --- AUTO VOICE SELECTION (EN/FR) ---
# If ARIA_TTS_VOICE_MAP is set: pick voice per chunk from the map (en/fr).
# Else: fallback to ARIA_TTS_VOICE (current behavior).
ARIA_TTS_VOICE_MAP=en:/opt/piper/voices/en_US-lessac-medium.onnx,fr:/opt/piper/voices/fr_FR-siwis-medium.onnx
ARIA_TTS_LANG_DETECT=heuristic
ARIA_TTS_LANG_SHORT_REUSE_CHARS=20

ARIA_TTS_RATE=1.0

# Chunking to reduce perceived latency
ARIA_TTS_CHUNKING=1
ARIA_TTS_MAX_CHARS_PER_CHUNK=220
ARIA_TTS_MIN_CHUNK_CHARS=60

# Output sink
ARIA_TTS_SINK=sonos_http

# Sonos control
ARIA_SONOS_IP=<sonos_speaker_ip>                 # e.g. 192.168.100.222

# HTTP server for Sonos to fetch audio (must be reachable by Sonos)
# Single-server deployment: use the main API host/port.
ARIA_HTTP_BASE_URL=http://<aria_server_ip>:8000  # e.g. http://192.168.100.100:8000

# LLM response length/verbosity tuning
ARIA_LLM_NUM_PREDICT=48
ARIA_LLM_TEMPERATURE=0.1
ARIA_LLM_TOP_P=0.85
ARIA_LLM_SYSTEM_PROMPT=You are ARIA, a local assistant running on a private server. Answer in 1-2 sentences. Be concise and direct.

# LLM conversation history (number of recent messages, user+assistant)
ARIA_LLM_HISTORY_MAX_MESSAGES=8

# Caching
ARIA_TTS_CACHE_DIR=/tmp/aria_tts_cache
ARIA_TTS_TIMEOUT_SEC=10

# Default volume scaling applied at normalization time (0.0–2.0)
ARIA_TTS_VOLUME_DEFAULT=0.3
```

### Echo Suppression v2 (Option A: event-aligned scoring)

Requirements:
- Maintain ONLY the CURRENT and LAST TTS events (max 2).
- Each event stores:
  - tts_id (uuid)
  - text (exact chunk text sent to TTS)
  - start_ts
  - end_ts_est (estimated)
- When Sonos playback is triggered: set `speaking=True`.
- When `now_ts > end_ts_est + tail_sec` and queue is empty: set `speaking=False`.

Echo suppression decision:
- On FINAL STT transcript, before calling the LLM:
  - If `speaking` is True OR within `tail_sec` of the most recent event:
    - Compute echo-likelihood score comparing transcript to:
      - CURRENT event text (if any)
      - LAST event text (if any)
    - If echo-likelihood is high → SUPPRESS (do not call LLM)
  - Otherwise allow transcript.

Non-goals:
- Do not compare against long TTS history.
- Do not attempt acoustic echo cancellation.

Env vars:
```
ARIA_ECHO_V2_ENABLED=1
ARIA_ECHO_V2_TAIL_SEC=2.0
ARIA_ECHO_V2_MIN_WORDS=5
ARIA_ECHO_V2_MIN_CHARS=20
ARIA_ECHO_V2_CONTAINMENT_THRESHOLD=0.65
ARIA_ECHO_V2_FUZZY_THRESHOLD=0.75
ARIA_ECHO_V2_ALLOWLIST_REGEX=\b(stop|cancel|aria|tais[- ]toi|stoppe)\b
```

Normalization:
- lowercase
- strip accents (é→e etc.)
- remove punctuation
- collapse whitespace
- OPTIONAL: drop digits or map digits to spaces

Scoring definition:
- Let `T` = normalized transcript, `S` = normalized candidate TTS chunk
- token containment:
  - containment = (# tokens in T that also appear in tokens(S)) / (# tokens in T)
  - ignore tokens shorter than 2 chars
  - ignore a small en/fr stopword list
- fuzzy token-set similarity:
  - fuzzy = token_set_ratio(T, S) / 100.0 (rapidfuzz if available, else fallback)

Decision:
- Suppress only if:
  - transcript length >= min_words/min_chars
  - transcript does NOT match allowlist regex
  - and (containment >= threshold_containment AND fuzzy >= threshold_fuzzy)

Logging:
- For every FINAL transcript, log one of:
  - ARIA.ECHO_V2: allowed {info}
  - ARIA.ECHO_V2: suppressed {info}
- info includes containment, fuzzy, matched current/last, transcript length, candidate length, tts_id.

### LLM Conversation History
- Aria keeps an **in-memory per-session history** and includes it in LLM requests.
- History is **cleared on client disconnect**.
- Configure size with `ARIA_LLM_HISTORY_MAX_MESSAGES` (user+assistant messages).
   - Suppress if:
     - Sequence similarity ≥ 0.85 OR
     - Jaccard word overlap ≥ 0.70
   - Allow partial suppression if:
     - transcript is long AND similarity ≥ 0.78

4) Do NOT suppress:
   - very short utterances
   - transcripts containing interrupt keywords: `stop`, `cancel`, `aria`

This mechanism is a **safety backstop**. It prevents feedback loops but does not replace VAD gating.

---

### MODULES TO CREATE / UPDATE

#### 1) `app/tts/tts_engine.py`
- Interface:
  - `synthesize_to_wav(text: str, *, voice: str, rate: float) -> Path`
- Responsible only for generating **raw** WAV via the selected engine.

#### 2) `app/tts/piper_engine.py`
- Piper-based implementation
- Cache raw outputs by hash(text + voice + rate)
- Enforce timeouts
- Fail gracefully with actionable logs

#### 3) `app/tts/audio_normalizer.py` (NEW)
- Convert raw WAV → Sonos-friendly WAV
- Uses `ffmpeg` subprocess
- Interface:
  - `normalize_for_sonos(in_path: Path, out_path: Path, volume: float) -> Path`
- Required normalization:
  - `-ac 2 -ar 44100 -sample_fmt s16`
  - volume filter: `-filter:a volume=<volume>`

#### 4) `app/tts/text_chunker.py`
- Split text into speakable chunks
- Prefer sentence boundaries
- Replace code blocks with short spoken summaries
- Avoid speaking long URLs

#### 5) `app/http_audio/server.py` (NEW) — HTTP audio endpoint (same Aria server)
- Expose cached/normalized audio for Sonos to fetch
- Must serve stable URLs, e.g.:
  - `GET /tts/<key>.wav` → returns normalized WAV
- Must not delete files while Sonos is fetching them
- Set `Content-Type: audio/wav`
- Bind to the main API host/port (single-server)

> Implementation hint: FastAPI + Uvicorn, or Starlette static file serving.
> This HTTP server runs **in the same process / deployment** as the Aria server (same port).

#### 6) `app/audio_sink/base.py`
- Sink interface:
  - `play_url(url: str) -> Handle`
  - `stop(handle)`
  - `stop_all()`

#### 7) `app/audio_sink/sonos_http_sink.py` (NEW)
- Use **SoCo**
- Minimal required methods:
  - `play_url(url)` → `SoCo(ARIA_SONOS_IP).play_uri(url)`
  - `stop_all()` → `SoCo(ARIA_SONOS_IP).stop()`
- Async-friendly: run network calls in a thread executor if needed.

#### 8) `app/speech_output/speech_output.py`
- Orchestrates:
  - chunking
  - TTS generation (raw)
  - normalization (Sonos-ready)
  - caching (normalized files)
  - URL construction (`ARIA_HTTP_BASE_URL + /tts/<key>.wav`)
  - sequential playback via `sonos_http_sink`
  - barge-in interrupt
- Public API:
  - `speak(text: str, volume: float | None = None)`
  - `interrupt()`

Auto voice selection (EN/FR) requirements:
- MUST support auto voice selection when `ARIA_TTS_VOICE_MAP` is set.
- Must parse `ARIA_TTS_VOICE_MAP` into a dict: `{ "en": "...", "fr": "..." }`.
- Must read `ARIA_TTS_LANG_DETECT` (default `heuristic`) and `ARIA_TTS_LANG_SHORT_REUSE_CHARS` (default `20`).
- For EACH speech chunk:
  a) If `len(text.strip()) < short_reuse_chars`: use `last_lang` (persisted across chunks)
  b) Else detect language -> `"en"` or `"fr"`
  c) Select voice = `voice_map[lang]` (fallback to `voice_map["en"]` if missing)
  d) Pass selected voice into the Piper synthesis call for that chunk.
- If `ARIA_TTS_VOICE_MAP` is NOT set: do NOT change existing logic; keep using `ARIA_TTS_VOICE`.

---

### Integration Flow (single server)

On FINAL transcript:
1. Print transcript
2. Call LLM
3. Print LLM output
4. If `ARIA_TTS_ENABLED=1`:
   - `speech_output.speak(llm_response)`

Notes:
- `speech_output.speak()` must ensure the normalized audio exists in cache **before** returning the URL to Sonos.
- Prefer sentence-level chunking so playback can start quickly while later chunks are still being synthesized.

Optional logs:
```
ARIA.TTS: speaking <n> chunks
ARIA.TTS: url=<http_url>
ARIA.TTS.ERROR: <error>
```

---

### BARGE-IN (PARKED)
- Barge-in is deferred; do not implement VAD-driven interrupts yet.

---

### Non-functional
- Entirely local
- Errors must not crash the server
- Easy to disable via env var
- Low latency via:
  - chunking
  - caching
  - pipelined synth/normalize/playback
- Avoid constant live streams; prefer **on-demand cached clips** + sequential play.

## APPENDIX — Fast EN/FR language detection heuristic (TTS)

Heuristic (no dependencies):
- Lowercase the text.
- If it contains any French diacritics: [àâäçéèêëîïôöùûüÿœæ] -> return "fr"
- Otherwise compute FR score and EN score by counting occurrences of common function words (word-boundary matches recommended).
  - FR markers:
    le la les un une des du de est sont être avoir je tu il elle nous vous ils elles avec pour sur dans mais donc
  - EN markers:
    the a an is are be have i you he she we they with for on in but so
- Return "fr" if fr_score > en_score else "en"
- Apply the short-text stability rule: if len(text.strip()) < ARIA_TTS_LANG_SHORT_REUSE_CHARS reuse last detected language.

Optional langid mode:
- If ARIA_TTS_LANG_DETECT=langid, then use the Python langid library (classify) and treat "fr" as French, else default to "en".
- If langid is unavailable at runtime, fallback to heuristic.
